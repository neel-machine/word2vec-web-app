{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Generate word embedding from text**\n",
        "\n",
        "In this notebook I have used the text from game of thrones books to create word embeddings using Word2Vec algorithm. \n",
        "\n",
        "Gensim is used to access Word2Vec model and further to save and load word embedding in flask file. The end goal is to get most similar words for a given text.\n",
        "\n",
        "Word embeddings are vectore representations of words in text that capture some context of the words. Unlike bag of words representation which result in large sparse vectors word embeddings are an improvement . \n",
        "\n",
        "Next step in this project is to plot the embeddings and observe the similar words that are placed nearer in the vector space. "
      ],
      "metadata": {
        "id": "X5u7P5UIR5qf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KDKK4KZd9v3",
        "outputId": "429bc29a-7d54-424d-b4fb-2f366b89cdcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.6)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.7.3)\n"
          ]
        }
      ],
      "source": [
        "pip install gensim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rsf-qc0KRhLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzIGzW4TgIdE",
        "outputId": "5c122c29-54cd-47a3-d5b4-e043352e716f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os,nltk\n",
        "from nltk import sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.utils import simple_preprocess\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7Dpp0G1eoBW",
        "outputId": "d59f75c6-d3f8-4867-d28e-3dee1e945684"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generate words from text**\n",
        "\n",
        "Here we traverse through the 5 copies of the text and process them into words that can be passed into word2vec model"
      ],
      "metadata": {
        "id": "VjYXE_YPTmCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = []\n",
        "stops = stopwords.words(\"english\")\n",
        "for filename in os.listdir('Data'):\n",
        "  f = open(os.path.join('Data',filename),encoding='ISO-8859-1')\n",
        "  text = f.read()\n",
        "  raw_sent = sent_tokenize(text)\n",
        "\n",
        "  for sent in raw_sent:\n",
        "    all_words.append(simple_preprocess(sent))"
      ],
      "metadata": {
        "id": "pmff0uUse50T"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec"
      ],
      "metadata": {
        "id": "lIrG0-TUgkHh"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eEWfWlLLWKUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Parameters for word2vec model**\n",
        "\n",
        "**size**: (default 100) The number of dimensions of the embedding, e.g. the length of the dense vector to represent each token (word).\n",
        "\n",
        "**window**: (default 5) The maximum distance between a target word and words around the target word.\n",
        "\n",
        "**min_count**: (default 5) The minimum count of words to consider when training the model; words with an occurrence less than this count will be ignored.\n",
        "\n",
        "**workers**: (default 3) The number of threads to use while training.\n",
        "\n",
        "**sg**: (default 0 or CBOW) The training algorithm, either CBOW (0) or skip gram (1)."
      ],
      "metadata": {
        "id": "YuVPaTRTWDvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_got = Word2Vec(all_words,min_count=5,window=10)"
      ],
      "metadata": {
        "id": "r1o-nZ05grzG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get the embedding vector for a token \n",
        " "
      ],
      "metadata": {
        "id": "REzuVV4VWypz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_got['daenerys']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zr5FnD4dh2XG",
        "outputId": "ad9d4f81-797f-415f-ac88-e357714b3218"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.00550811,  0.1979751 ,  0.45436585, -0.17187944, -1.5095897 ,\n",
              "       -0.12588347,  0.5069664 , -0.472151  ,  0.5028591 , -0.52353543,\n",
              "       -0.43614462, -1.0732169 , -0.1719547 ,  1.0185014 , -0.08529336,\n",
              "        0.76177436,  0.65787834, -0.96632373,  0.2659897 ,  0.34751108,\n",
              "       -0.3727839 , -0.44544104, -0.58361703, -0.4645536 ,  1.0248919 ,\n",
              "       -0.18955937,  1.0321758 ,  0.31095877,  1.0237343 ,  0.6839877 ,\n",
              "       -0.60695994, -0.5354355 , -1.2755035 ,  0.7971668 , -0.6465591 ,\n",
              "        0.08155406,  0.8134394 ,  0.6021161 , -1.0907316 ,  0.81290597,\n",
              "       -0.9022983 , -0.5896426 ,  0.6111805 ,  0.17473985,  0.06486788,\n",
              "        0.2494802 ,  0.8075841 ,  0.23277532, -0.70357394,  0.00439382,\n",
              "       -1.2981724 , -0.36704057, -0.25806957,  0.36551657,  0.08756131,\n",
              "        0.9101219 ,  0.4230666 ,  0.49521732,  0.568394  , -0.24985427,\n",
              "        1.3264594 , -0.4367455 , -0.09210678, -0.8864302 , -0.01741638,\n",
              "       -0.11204147,  0.01746313, -0.00854641,  0.0116835 , -0.05726743,\n",
              "       -0.54433626,  0.22904775, -1.3406897 , -0.6610953 , -0.6272306 ,\n",
              "       -1.0032845 ,  0.53418463,  0.4606925 ,  0.93822604,  0.7281666 ,\n",
              "       -0.14789079,  0.04051362, -0.24510467, -0.24634029, -1.0460657 ,\n",
              "       -0.8804115 , -0.4640107 ,  0.36779842,  0.25811005,  0.30809748,\n",
              "        0.42933074, -0.73109686,  0.53956574, -0.47456834, -0.28694293,\n",
              "        0.05908213, -0.55142105, -0.7742792 , -0.39838415, -0.5460495 ],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_got.most_similar('daenerys')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_gW4ofliNrf",
        "outputId": "d0fe05b7-4169-4430-9b0f-fe7bb065ee79"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('stormborn', 0.7624735832214355),\n",
              " ('targaryen', 0.7419289350509644),\n",
              " ('queen', 0.7079192996025085),\n",
              " ('princess', 0.6999870538711548),\n",
              " ('myrcella', 0.6607555747032166),\n",
              " ('elia', 0.6533308029174805),\n",
              " ('dorne', 0.6479859352111816),\n",
              " ('viserys', 0.6382139921188354),\n",
              " ('margaery', 0.6318345665931702),\n",
              " ('unburnt', 0.6315217018127441)]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model_got.wv.save_word2vec_format('model_got.bin')"
      ],
      "metadata": {
        "id": "TkaKSgMWilph"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To save the model in binary format. I exported this binary file to flask app in the later steps."
      ],
      "metadata": {
        "id": "lfV5eEYvWt4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_got.save('model_embeddings.bin')"
      ],
      "metadata": {
        "id": "hKSKpYecsNA6"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1uUF2ysfRgDa"
      }
    }
  ]
}